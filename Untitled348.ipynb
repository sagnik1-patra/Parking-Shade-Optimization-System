{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d6885b5-b9e3-4bf8-9856-bb85dc0fbce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "17959/17959 [==============================] - 57s 3ms/step - loss: 92.2279 - val_loss: 0.0072\n",
      "Epoch 2/20\n",
      "17959/17959 [==============================] - 57s 3ms/step - loss: 0.0143 - val_loss: 0.1483\n",
      "Epoch 3/20\n",
      "17959/17959 [==============================] - 62s 3ms/step - loss: 0.0132 - val_loss: 9.6157e-04\n",
      "Epoch 4/20\n",
      "17959/17959 [==============================] - 61s 3ms/step - loss: 0.0129 - val_loss: 0.2069\n",
      "Epoch 5/20\n",
      "17959/17959 [==============================] - 57s 3ms/step - loss: 0.0125 - val_loss: 7.2703e-04\n",
      "Epoch 6/20\n",
      "17959/17959 [==============================] - 59s 3ms/step - loss: 0.0122 - val_loss: 0.0076\n",
      "Epoch 7/20\n",
      "17959/17959 [==============================] - 54s 3ms/step - loss: 0.0120 - val_loss: 0.0088\n",
      "Epoch 8/20\n",
      "17959/17959 [==============================] - 53s 3ms/step - loss: 0.0117 - val_loss: 0.0253\n",
      "Epoch 9/20\n",
      "17959/17959 [==============================] - 55s 3ms/step - loss: 0.0117 - val_loss: 0.0052\n",
      "Epoch 10/20\n",
      "17959/17959 [==============================] - 57s 3ms/step - loss: 0.0115 - val_loss: 0.0160\n",
      "9977/9977 [==============================] - 18s 2ms/step\n",
      "✅ All files saved successfully in: C:\\Users\\NXTWAVE\\Downloads\\Parking Shade Optimization System\\archive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NXTWAVE\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# ============================================================\n",
    "# PATHS\n",
    "# ============================================================\n",
    "\n",
    "BASE_DIR = r\"C:\\Users\\NXTWAVE\\Downloads\\Parking Shade Optimization System\\archive\"\n",
    "\n",
    "FILES = {\n",
    "    \"city\": \"city_attributes.csv\",\n",
    "    \"humidity\": \"humidity.csv\",\n",
    "    \"pressure\": \"pressure.csv\",\n",
    "    \"temperature\": \"temperature.csv\",\n",
    "    \"weather\": \"weather_description.csv\",\n",
    "    \"wind_dir\": \"wind_direction.csv\",\n",
    "    \"wind_speed\": \"wind_speed.csv\"\n",
    "}\n",
    "\n",
    "# ============================================================\n",
    "# LOAD & MERGE DATA\n",
    "# ============================================================\n",
    "\n",
    "dfs = {}\n",
    "for k, v in FILES.items():\n",
    "    dfs[k] = pd.read_csv(os.path.join(BASE_DIR, v))\n",
    "\n",
    "# Convert wide → long format for time-series merge\n",
    "def melt_df(df, value_name):\n",
    "    return df.melt(id_vars=[\"datetime\"], var_name=\"city\", value_name=value_name)\n",
    "\n",
    "humidity = melt_df(dfs[\"humidity\"], \"humidity\")\n",
    "pressure = melt_df(dfs[\"pressure\"], \"pressure\")\n",
    "temperature = melt_df(dfs[\"temperature\"], \"temperature\")\n",
    "wind_speed = melt_df(dfs[\"wind_speed\"], \"wind_speed\")\n",
    "wind_dir = melt_df(dfs[\"wind_dir\"], \"wind_direction\")\n",
    "\n",
    "# Merge all\n",
    "df = humidity.merge(pressure, on=[\"datetime\", \"city\"])\n",
    "df = df.merge(temperature, on=[\"datetime\", \"city\"])\n",
    "df = df.merge(wind_speed, on=[\"datetime\", \"city\"])\n",
    "df = df.merge(wind_dir, on=[\"datetime\", \"city\"])\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# ============================================================\n",
    "# FEATURE ENGINEERING\n",
    "# ============================================================\n",
    "\n",
    "df[\"shade_need_index\"] = (\n",
    "    df[\"temperature\"] * 0.5 +\n",
    "    df[\"humidity\"] * 0.2 -\n",
    "    df[\"wind_speed\"] * 0.3\n",
    ")\n",
    "\n",
    "features = [\"humidity\", \"pressure\", \"temperature\", \"wind_speed\", \"wind_direction\"]\n",
    "target = \"shade_need_index\"\n",
    "\n",
    "X = df[features].values\n",
    "y = df[target].values\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X_scaled = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# CNN MODEL\n",
    "# ============================================================\n",
    "\n",
    "model = Sequential([\n",
    "    Conv1D(32, 2, activation=\"relu\", input_shape=(X_scaled.shape[1], 1)),\n",
    "    Conv1D(64, 2, activation=\"relu\"),\n",
    "    Flatten(),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[EarlyStopping(patience=5)],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "preds = model.predict(X_test).flatten()\n",
    "mse = mean_squared_error(y_test, preds)\n",
    "r2 = r2_score(y_test, preds)\n",
    "\n",
    "# ============================================================\n",
    "# PSO OPTIMIZATION (Shade Placement Score)\n",
    "# ============================================================\n",
    "\n",
    "def fitness(position):\n",
    "    return np.sum(position * np.mean(X, axis=0))\n",
    "\n",
    "particles = 30\n",
    "dimensions = len(features)\n",
    "iterations = 50\n",
    "\n",
    "swarm = np.random.rand(particles, dimensions)\n",
    "velocity = np.zeros_like(swarm)\n",
    "\n",
    "pbest = swarm.copy()\n",
    "pbest_score = np.array([fitness(p) for p in swarm])\n",
    "\n",
    "gbest = pbest[np.argmin(pbest_score)]\n",
    "\n",
    "for _ in range(iterations):\n",
    "    for i in range(particles):\n",
    "        r1, r2 = random.random(), random.random()\n",
    "        velocity[i] = (\n",
    "            0.5 * velocity[i]\n",
    "            + r1 * (pbest[i] - swarm[i])\n",
    "            + r2 * (gbest - swarm[i])\n",
    "        )\n",
    "        swarm[i] += velocity[i]\n",
    "\n",
    "        score = fitness(swarm[i])\n",
    "        if score < pbest_score[i]:\n",
    "            pbest[i] = swarm[i]\n",
    "            pbest_score[i] = score\n",
    "\n",
    "    gbest = pbest[np.argmin(pbest_score)]\n",
    "\n",
    "# ============================================================\n",
    "# SAVE OUTPUTS\n",
    "# ============================================================\n",
    "\n",
    "# H5 Model\n",
    "model.save(os.path.join(BASE_DIR, \"parking_shade_cnn_model.h5\"))\n",
    "\n",
    "# PKL\n",
    "with open(os.path.join(BASE_DIR, \"parking_shade_objects.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\"scaler\": scaler, \"pso_best\": gbest}, f)\n",
    "\n",
    "# YAML Config\n",
    "config = {\n",
    "    \"model\": \"CNN + PSO\",\n",
    "    \"features\": features,\n",
    "    \"epochs\": 20,\n",
    "    \"particles\": particles,\n",
    "    \"iterations\": iterations\n",
    "}\n",
    "\n",
    "with open(os.path.join(BASE_DIR, \"parking_shade_config.yaml\"), \"w\") as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "# JSON Results\n",
    "results = {\n",
    "    \"mse\": float(mse),\n",
    "    \"r2_score\": float(r2),\n",
    "    \"pso_best_weights\": gbest.tolist(),\n",
    "    \"sample_predictions\": preds[:10].tolist()\n",
    "}\n",
    "\n",
    "with open(os.path.join(BASE_DIR, \"parking_shade_results.json\"), \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"✅ All files saved successfully in:\", BASE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468a5eb8-bea6-4bcb-9296-7120342da6f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
